{
    "docs": [
        {
            "location": "/",
            "text": "tmap Documentation\n\n\nFor large scale and integrative microbiome research, it is expected to apply advanced data mining techniques in microbiome analysis.\n\n\nTopological data analysis(TDA) provides a promising technique for analyzing large scale complex data. The most popular \nMapper\n algorithm is effective in distilling data-shape from high dimensional space, and provides a compressive network representation.\n\n\ntmap\n is a topological data analysis framework implemented of the \nTDA Mapper\n for population-scale microbiome data analysis. We developed \ntmap\n to enable easy adoption of TDA in microbiome data analysis pipeline,providing network-based statistical methods for enterotype analysis, driver species identification, and microbiome-wide association of host meta-data.",
            "title": "tmap"
        },
        {
            "location": "/#tmap-documentation",
            "text": "For large scale and integrative microbiome research, it is expected to apply advanced data mining techniques in microbiome analysis.  Topological data analysis(TDA) provides a promising technique for analyzing large scale complex data. The most popular  Mapper  algorithm is effective in distilling data-shape from high dimensional space, and provides a compressive network representation.  tmap  is a topological data analysis framework implemented of the  TDA Mapper  for population-scale microbiome data analysis. We developed  tmap  to enable easy adoption of TDA in microbiome data analysis pipeline,providing network-based statistical methods for enterotype analysis, driver species identification, and microbiome-wide association of host meta-data.",
            "title": "tmap Documentation"
        },
        {
            "location": "/basic/",
            "text": "Basic Usage of tmap\n\n\nThis guide can help you start working with tmap.\n\n\nIn our daily life, facing high dimension data and complex interaction data always is the most common things in data processing. For visualization, we need dimensionality reduction techniques or ordiantion analysis. For data processing, we need to reduce dimension to meet the requirement of algorithms or the limitation of machines.\n\n\nThe \ntmap\n is a method which can be used to reduce high dimensional data sets into simplicial complexes with far fewerpoints which can capture topological and geometric information at a specified resolution.\n\n\nLet start the simplest case first - assuming we have data in nice tidy dataframe format.\n\n\nThe Simple Case\n\n\nUsing the classical iris dataset as a simple example.\n\n\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data\n\n\n\n\nOnce we get a data and we need to initiate some basic instance such as \nMapper\n, \nfilter\n, \ncluster\n, \nCover\n.\n\n\nfrom tmap.tda import mapper, filter\nfrom tmap.tda.cover import Cover\n\n# Step1. initiate a Mapper\ntm = mapper.Mapper(verbose=1)\n\n# Step2. Projection\nlens = [filter.MDS(components=[0, 1])]\nprojected_X = tm.filter(X, lens=lens)\nclusterer = DBSCAN(eps=0.75, min_samples=1)\ncover = Cover(projected_data=MinMaxScaler().fit_transform(projected_X), resolution=20, overlap=0.75)\n\n\n\n\nThe data you provide to \nCover\n is suggested to be transformed first.\n\n\nAfter preparing all instance and data,all you need to do is pass data, \nCover\n, \ncluster\n to \nmap\n and get a returned object. A \ngraph\n is a collection of nodes (vertices) along with identified pairs of nodes (called edges). For now, we use nest dictionary as container to stored all information of graph.\n\n\nFlexible and portable classes will be implement for graph at following work.\n\n\ngraph = tm.map(data=StandardScaler().fit_transform(X), cover=cover, clusterer=clusterer)\n\n\n\n\nWith this process, the grapp consists of 201 nodes and 1020 edges, as can be seen by:\n\n\nprint(len(graph['nodes']),len(graph['edges']))\n\n201 1020\n\n\n\n\nEach nodes containing several samples from origin data. Mapping relationship between nodes and samples could be found by:\n\n\nprint(graph['nodes'].items())\n(0, array([ 718, 1655, 2564, 2846, 3946, 4431])),\n(1, array([  69,  616,  853, 1266, 1575, 1833, 1959, 2268, 3635])),\n(2, array([  28,  350,  511,  601,  616,  629,  723, 1162, 1193, 1266,\n  1300,1424, 1536, 1634, 1841, 1868, 1943, 2268, 2282, 2719, 2898,\n  2929,3300, 3605, 3634, 3763, 3816, 3848, 4461, 4828])),\n(3, array([   1,  669,  692,  814, 1064, 1424, 1435, 1503, 1512, 1909, 2145,\n  2182, 2412, 2786, 2840, 2849, 2929, 3132, 3162, 3179, 3414, 3439,\n  3541, 3667, 3708, 3759, 4038, 4168, 4170, 4176, 4478, 4497, 4606,\n  4938, 4974])),\n......\n\n\n\n\nUsing different distance metric\n\n\nAfter introducing basic usage of tmap, a little more details of each classes will be discuessed. We may want to using different distance metric instead of default(Euclidean) distance metric frequently. Particularly in microbiome data analysis, most popular distance metric we used is weighted or unweighted unifrac.\n\n\nFor using custom distance metric, you need to set a parameter \nmetric\n as \n\"precomputed\"\n in \nfilter\n.\n\n\nlens = [filter.MDS(components=[0, 1],metric='precomputed')]\n#distance = pd.read_csv('precomputed_distance.csv')\nprojected_X = tm.filter(dis, lens=lens)\n\n\n\n\nThe components of \nfilter\n is also could be passed other PC for PCA or MDS. More filter will be complemented at following version.\n\n\nfilter\n is general method which is trying to reduce the dimension of data for decrease computation time. If there are two points are nearby in high-dimension space, \nfilter\n with false metric or projection makes them separated far beyond each other and located in a different hyper cube. In this situation, it will make them clustering into two nodes and becoming a false positive case. If we have two distant points in origin distribution, this will not be clustering into one node because clustering is based on origin distance.\n\n\nVisualization\n\n\nAfter constructing a graph, we should try to visualize the network into 2d graph. There are multiple tools in python to representing a graph with nodes and edges, such as \nnetworkx\n,\nGraphviz\n,\ngraph-tool\n. It could design some wrappers with these tools or even using web-based forced layout html to display the graph. But currently, we just build a simplest wrapper with \nnetworkx\n and \nmatplolib\n to visualize the graph.\n\n\nBesides projecting the network, we also design a \nColor\n class to annotate each node with providing features.\n\n\nfrom tmap.tda.plot import show, Color\ny = iris.target\ncolor = Color(target=y, dtype=\"categorical\")\nshow(data=X, graph=graph, color=color, fig_size=(10, 10), node_size=15, mode='spring', strength=0.04)\n\n\n\n\n\n\nDepending on the type of target data, there are two type \ncategorical\n and \nnumerical\n we can choose. If we have a binary feature or continuous feature, we recommend \nnumerical\n to show the distribution of data. For a binary feature, the value of a node indicating a ratio of True/False for the feature. For a multi-classes feature, you could use \ncategorical\n to visualize most abundance category in each node. But you could also use \nOne-Hot encoded\n method to transform a multi-classes feature into a binary feature.\n\n\nSAFE scores\n\n\nAfter visualization, we should find a way to analyze the graph.\n\n\nFrom original data, we just need the distance between each sample to get the graph and visualize it. The graph without color is just showing the coherent of samples. When we colorize the graph with abundance data, each feature is representing different pattern along with the graph.\n\n\nSAFE score is a algorithm to reveal the pattern of features.\n\n\nPlotting the first feature of iris dataset and colorize it with abundance data.\n\n\ncolor = Color(target=X[:,0], dtype=\"numerical\")\nshow(data=X, graph=graph, color=color, fig_size=(10, 10), node_size=15, mode='spring', strength=0.04)\n\n\n\n\n\n\nThe continuous of color showing that this feature is strongly associated with the graph. Next step is using a quantitative algorithm to represent it.\n\n\nfrom tmap.netx.SAFE import *\nsafe_scores = SAFE_batch(graph, meta_data=X, n_iter=1000, threshold=0.05)\ncolor = Color(target=safe_scores[X.columns[0]], dtype=\"numerical\")\nshow(data=X, graph=graph, color=color, fig_size=(10, 10), node_size=15, mode='spring', strength=0.04)\n\n\n\n\n\n\nPlease see the \nSAFE algorithm\n for more details.\n\n\nSAFE summary\n\n\nSAFE score is not only used as a scale for color, it also should ranking features to screen or filter important features for the network which reflects inner relationship of all samples.\n\n\nfrom tmap.netx.SAFE import get_SAFE_summary\n\nsafe_summary = get_SAFE_summary(graph=graph, meta_data=X, safe_scores=safe_scores,\n                                n_iter_value=1000, p_value=0.01)\n\n\n\n\nTaking p-value 0.01 as threshold to count all significant nodes with higher SAFE score could provide some variable called \nSAFE enriched score , enriched SAFE score ratio\n to ranking the importance or significance of features.\n\n\nMore detailed about SAFE summary should refer to \nblablabla\n.\n\n\nCo-enrichment scores\n\n\nBesides the association with the cohort, correlation between species and meta data also is a requisite result that we are pursuing.\n\n\nIn tmap, SAFE score of nodes is taking as a new measurement to reveal this kind of relationship between any features.\n\n\nWith SAFE score and corresponding graph, p-value and its correlation coefficient of each pair of features are calculated by pearson test and corrected by Benjamini/Hochberg correction.\n\n\ncoenrich_edges = coenrich_v2(graph,safe_scores)\n\nprint(coenrich_edges)",
            "title": "Basic Usage"
        },
        {
            "location": "/basic/#basic-usage-of-tmap",
            "text": "This guide can help you start working with tmap.  In our daily life, facing high dimension data and complex interaction data always is the most common things in data processing. For visualization, we need dimensionality reduction techniques or ordiantion analysis. For data processing, we need to reduce dimension to meet the requirement of algorithms or the limitation of machines.  The  tmap  is a method which can be used to reduce high dimensional data sets into simplicial complexes with far fewerpoints which can capture topological and geometric information at a specified resolution.  Let start the simplest case first - assuming we have data in nice tidy dataframe format.",
            "title": "Basic Usage of tmap"
        },
        {
            "location": "/basic/#the-simple-case",
            "text": "Using the classical iris dataset as a simple example.  from sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data  Once we get a data and we need to initiate some basic instance such as  Mapper ,  filter ,  cluster ,  Cover .  from tmap.tda import mapper, filter\nfrom tmap.tda.cover import Cover\n\n# Step1. initiate a Mapper\ntm = mapper.Mapper(verbose=1)\n\n# Step2. Projection\nlens = [filter.MDS(components=[0, 1])]\nprojected_X = tm.filter(X, lens=lens)\nclusterer = DBSCAN(eps=0.75, min_samples=1)\ncover = Cover(projected_data=MinMaxScaler().fit_transform(projected_X), resolution=20, overlap=0.75)  The data you provide to  Cover  is suggested to be transformed first.  After preparing all instance and data,all you need to do is pass data,  Cover ,  cluster  to  map  and get a returned object. A  graph  is a collection of nodes (vertices) along with identified pairs of nodes (called edges). For now, we use nest dictionary as container to stored all information of graph.  Flexible and portable classes will be implement for graph at following work.  graph = tm.map(data=StandardScaler().fit_transform(X), cover=cover, clusterer=clusterer)  With this process, the grapp consists of 201 nodes and 1020 edges, as can be seen by:  print(len(graph['nodes']),len(graph['edges']))\n\n201 1020  Each nodes containing several samples from origin data. Mapping relationship between nodes and samples could be found by:  print(graph['nodes'].items())\n(0, array([ 718, 1655, 2564, 2846, 3946, 4431])),\n(1, array([  69,  616,  853, 1266, 1575, 1833, 1959, 2268, 3635])),\n(2, array([  28,  350,  511,  601,  616,  629,  723, 1162, 1193, 1266,\n  1300,1424, 1536, 1634, 1841, 1868, 1943, 2268, 2282, 2719, 2898,\n  2929,3300, 3605, 3634, 3763, 3816, 3848, 4461, 4828])),\n(3, array([   1,  669,  692,  814, 1064, 1424, 1435, 1503, 1512, 1909, 2145,\n  2182, 2412, 2786, 2840, 2849, 2929, 3132, 3162, 3179, 3414, 3439,\n  3541, 3667, 3708, 3759, 4038, 4168, 4170, 4176, 4478, 4497, 4606,\n  4938, 4974])),\n......",
            "title": "The Simple Case"
        },
        {
            "location": "/basic/#using-different-distance-metric",
            "text": "After introducing basic usage of tmap, a little more details of each classes will be discuessed. We may want to using different distance metric instead of default(Euclidean) distance metric frequently. Particularly in microbiome data analysis, most popular distance metric we used is weighted or unweighted unifrac.  For using custom distance metric, you need to set a parameter  metric  as  \"precomputed\"  in  filter .  lens = [filter.MDS(components=[0, 1],metric='precomputed')]\n#distance = pd.read_csv('precomputed_distance.csv')\nprojected_X = tm.filter(dis, lens=lens)  The components of  filter  is also could be passed other PC for PCA or MDS. More filter will be complemented at following version.  filter  is general method which is trying to reduce the dimension of data for decrease computation time. If there are two points are nearby in high-dimension space,  filter  with false metric or projection makes them separated far beyond each other and located in a different hyper cube. In this situation, it will make them clustering into two nodes and becoming a false positive case. If we have two distant points in origin distribution, this will not be clustering into one node because clustering is based on origin distance.",
            "title": "Using different distance metric"
        },
        {
            "location": "/basic/#visualization",
            "text": "After constructing a graph, we should try to visualize the network into 2d graph. There are multiple tools in python to representing a graph with nodes and edges, such as  networkx , Graphviz , graph-tool . It could design some wrappers with these tools or even using web-based forced layout html to display the graph. But currently, we just build a simplest wrapper with  networkx  and  matplolib  to visualize the graph.  Besides projecting the network, we also design a  Color  class to annotate each node with providing features.  from tmap.tda.plot import show, Color\ny = iris.target\ncolor = Color(target=y, dtype=\"categorical\")\nshow(data=X, graph=graph, color=color, fig_size=(10, 10), node_size=15, mode='spring', strength=0.04)   Depending on the type of target data, there are two type  categorical  and  numerical  we can choose. If we have a binary feature or continuous feature, we recommend  numerical  to show the distribution of data. For a binary feature, the value of a node indicating a ratio of True/False for the feature. For a multi-classes feature, you could use  categorical  to visualize most abundance category in each node. But you could also use  One-Hot encoded  method to transform a multi-classes feature into a binary feature.",
            "title": "Visualization"
        },
        {
            "location": "/basic/#safe-scores",
            "text": "After visualization, we should find a way to analyze the graph.  From original data, we just need the distance between each sample to get the graph and visualize it. The graph without color is just showing the coherent of samples. When we colorize the graph with abundance data, each feature is representing different pattern along with the graph.  SAFE score is a algorithm to reveal the pattern of features.  Plotting the first feature of iris dataset and colorize it with abundance data.  color = Color(target=X[:,0], dtype=\"numerical\")\nshow(data=X, graph=graph, color=color, fig_size=(10, 10), node_size=15, mode='spring', strength=0.04)   The continuous of color showing that this feature is strongly associated with the graph. Next step is using a quantitative algorithm to represent it.  from tmap.netx.SAFE import *\nsafe_scores = SAFE_batch(graph, meta_data=X, n_iter=1000, threshold=0.05)\ncolor = Color(target=safe_scores[X.columns[0]], dtype=\"numerical\")\nshow(data=X, graph=graph, color=color, fig_size=(10, 10), node_size=15, mode='spring', strength=0.04)   Please see the  SAFE algorithm  for more details.",
            "title": "SAFE scores"
        },
        {
            "location": "/basic/#safe-summary",
            "text": "SAFE score is not only used as a scale for color, it also should ranking features to screen or filter important features for the network which reflects inner relationship of all samples.  from tmap.netx.SAFE import get_SAFE_summary\n\nsafe_summary = get_SAFE_summary(graph=graph, meta_data=X, safe_scores=safe_scores,\n                                n_iter_value=1000, p_value=0.01)  Taking p-value 0.01 as threshold to count all significant nodes with higher SAFE score could provide some variable called  SAFE enriched score , enriched SAFE score ratio  to ranking the importance or significance of features.  More detailed about SAFE summary should refer to  blablabla .",
            "title": "SAFE summary"
        },
        {
            "location": "/basic/#co-enrichment-scores",
            "text": "Besides the association with the cohort, correlation between species and meta data also is a requisite result that we are pursuing.  In tmap, SAFE score of nodes is taking as a new measurement to reveal this kind of relationship between any features.  With SAFE score and corresponding graph, p-value and its correlation coefficient of each pair of features are calculated by pearson test and corrected by Benjamini/Hochberg correction.  coenrich_edges = coenrich_v2(graph,safe_scores)\n\nprint(coenrich_edges)",
            "title": "Co-enrichment scores"
        },
        {
            "location": "/param/",
            "text": "How to choose parameters in tmap\n\n\nWhile tmap has a simple usage way to map a high-dimension data into a 2d graph, in practice there are several parameters in tmap would influence the final graph. We will demonstrate how these parameters change would affect the final graph and how to choose them.\n\n\nSelecting \ncomponents\n in \nfilter\n\n\nThe primary parameter to effect the resulting is the \ncomponents\n in \nfilter\n. If we explore the algorithms of \ntmap\n in \nhow tmap work\n, we could know that increasing the number components of filter will exponential increase the time of calculation and also the number of edges/nodes. Besides, changing the way of projection or chosen components is altering the angle of view to data points clouds and it will also impact the final graph we extracted from original data.\n\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn import datasets\nfrom sklearn.cluster import DBSCAN\nfrom tmap.tda import mapper, filter\nfrom tmap.tda.cover import Cover\nfrom tmap.tda.plot import show, Color\n\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Step1. initiate a Mapper\ntm = mapper.Mapper(verbose=1)\n\n# Step2. Projection\nlens = [filter.MDS(components=[0, 1],random_state=100)]\nprojected_X = tm.filter(X, lens=lens)\n\n# Step3. Covering, clustering & mapping\nclusterer = DBSCAN(eps=0.75, min_samples=1)\ncover = Cover(projected_data=MinMaxScaler().fit_transform(projected_X), resolution=20, overlap=0.75)\ngraph = tm.map(data=StandardScaler().fit_transform(X), cover=cover, clusterer=clusterer)\n\nshow(data=X,graph=graph, color='b', fig_size=(10, 10), node_size=15, mode='spring', strength=0.03)\n\n\n\n\nFiltering by MDS.\n...calculate distance matrix using the euclidean metric.\nFinish filtering of points cloud data.\nMapping on data (150, 4) using lens (150, 2)\n...minimal number of points in hypercube to do clustering: 1\n...create 219 nodes.\n...calculate projection coordinates of nodes.\n...construct a TDA graph.\n...create 1133 edges.\nFinish TDA mapping\n\n\n\n\n\nUsing first two components of MDS as \nlens\n, graph with 219 nodes ans 1133 edges will be generated.\n\n\nWhat if we choose first three components of MDS?\n\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn import datasets\nfrom sklearn.cluster import DBSCAN\nfrom tmap.tda import mapper, filter\nfrom tmap.tda.cover import Cover\nfrom tmap.tda.plot import show\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Step1. initiate a Mapper\ntm = mapper.Mapper(verbose=1)\n\n# Step2. Projection\nlens = [filter.MDS(components=[0, 1,2],random_state=100)]\nprojected_X = tm.filter(X, lens=lens)\n\n# Step3. Covering, clustering & mapping\nclusterer = DBSCAN(eps=0.75, min_samples=1)\ncover = Cover(projected_data=MinMaxScaler().fit_transform(projected_X), resolution=20, overlap=0.75)\ngraph = tm.map(data=StandardScaler().fit_transform(X), cover=cover, clusterer=clusterer)\n\nshow(data=X,graph=graph, color='b', fig_size=(10, 10), node_size=15, mode='spring', strength=0.17)\n\n\n\n\nFiltering by MDS.\n...calculate distance matrix using the euclidean metric.\nFinish filtering of points cloud data.\nMapping on data (150, 4) using lens (150, 3)\n...minimal number of points in hypercube to do clustering: 1\n...create 921 nodes.\n...calculate projection coordinates of nodes.\n...construct a TDA graph.\n...create 13279 edges.\nFinish TDA mapping\n\n\n\n\n\n\nWe could get a graph with 921 nodes and 13279 edges. Increasing components would make the graph become bigger and redundant. If we visualize these two graphs, we could see that the general topological structure is similar. The more the number of \ncomponents\n you provide, the more complicated graph will be generated because of the dimensional problems. We recommend the default parameter \ncomponents=[0,1]\n in most situations.\n\n\nSelecting \nresolution\n and \noverlap\n in \nCover\n\n\nA further parameter that affects the resulting is about the \nCover\n which is a sifter for any given filter. \nresolution\n decides how many intervals you need to extract from the original point cloud based on the \nfilter\n in each axis. \noverlap\n influences how much space between each interval need to be overlapped.\n\n\nFor \nresolution\n, the number of hypercube equals to \nresolution ** n_components\n. Enlarging the parameter will highly increase the time of calculation but improve the precision of the graph. Theoretically, higher precision is better for retrieving more detail in the graph. But blindly enlarging \nresolution\n could get a catastrophe that the number of nodes is much more than the number of samples and no connections will emerge between any of nodes.\n\n\n\n\nIf we enlarge 10 times of the \nresolution\n  and leave the \noverlap\n unchanged will cause the situation above. High \nresolution\n for a small data set will cause each bin/hypercube has too few samples before clustering and no samples are located inside the overlap region. The resulting graph will become a network of discrete islands.\n\n\nThe smaller the \nresolution\n of cover, the less the graph could describe original points cloud. A suitable \nresolution\n could highly reduce the time and effort.\n\n\nFor \noverlap\n, it majority influences the edges between each node. The larger the \noverlap\n you provide, the more edges will emerge until reaching limitation. But too low \noverlap\n will lose some detail of origin data.\n\n\n\n\nAs we see above, \nresolution\n is an empirical param which is depended on the size of your data set. It should be carefully adjusted to depict the data set properly. The larger the \noverlap\n you provide, the more structure it will capture. But for visualization and calculation, we should choose a suitable value.\n\n\nApart from the general structure of the graph, these parameters also influence most \ncover ratio\n which is discussed below.\n\n\nSelecting \neps\n and \nmin_samples\n in DBSCAN\n\n\nIn tmap, default cluster called DBSCAN which requires 2 parameters - \neps\n, which restrict maximum distance between two samples for them to be considered as in the same cluster; and \nmin_samples\n, which specifies how many neighbors a point should have to be included into a cluster. However, you may not know these values in advance.\n\n\nHere we implemented a function called \nutils.optimize_dbscan_eps\n to easily adjust \neps\n.\n\n\nFor better performances, we should enlarge the \neps\n to cover most of the pairs. To achieve this, the shortest distance from each point to its nearest neighbor will be considered together. Collecting all distance could build a distribution and using a \nthreshold\n as a \npercentile\n to cut how many pairwise distance you want to cover. After passing a threshold to \noptimize_dbscan_eps\n, you could get a suitable \neps\n for DBSCAN algorithm.\n\n\nAfter we have chosen the value for \neps\n , we may wonder how many points lie within each point's epsilon-neighborhood.\n\n\nFor extracting the topological properties as much as possible, we should still consider the distribution construct by the number of neighbor of each point. But not like the \neps\n optimization function, we have not implemented this function yet. It may be implemented in the future.\n\n\nOptimization of \ncover ratio\n\n\ncover ratio\n is a variant for measuring how many samples are retaining or missing during the transform process from data to the graph.\n\n\nDuring the process, samples placed in a hyper cube without being clustered into any nodes will be discarded. Usually, it is taking as outlier or noise found in data.\n\n\nIf we don't consider \ncover ratio\n in tmap result, it could get a perfect result like overfitting when we drop too much samples. So \ncover ratio\n need to be considered\n\n\nrelationship between \ncover ratio\n with other parameters\n\n\nGiven different parameters to FGFP data in example would generate under figure.\n\n\n\nApparently, different parameters own different relationship with \ncover ratio\n and it is monotonous in given range.\n\n\n\n\n\n\n\n\nparamerters\n\n\nrelationship\n\n\n\n\n\n\n\n\n\n\neps threshold\n\n\nmonotonous increasing\n\n\n\n\n\n\noverlap\n\n\nmonotonous increasing\n\n\n\n\n\n\nresolution\n\n\nmonotonous decreasing\n\n\n\n\n\n\nmin samples\n\n\nmonotonous decreasing\n\n\n\n\n\n\n\n\nSummarize\n\n\ncover ratio\n is a variant which will be impacted by every parameters and so it is hard to tell what influence it will make instead of other parameters. In general, it may cover at least 80% samples like the explained proportion of PCA.\n\n\nIn the case of \ncover ratio\n of 80%, resolution may reach \nsqrt(n_samples*2)\n as far as possible. Other parameters may be chosen depending on different circumstances. Note that there are not best parameters in tmap because different features may present different topological shape in high dimensional space. Using different angle of view and resulting a weird graph doesn't mean that it is a wrong graph. How to benchmark the graph properly also is open to exploration and implement in the future.",
            "title": "Parameter Selection"
        },
        {
            "location": "/param/#how-to-choose-parameters-in-tmap",
            "text": "While tmap has a simple usage way to map a high-dimension data into a 2d graph, in practice there are several parameters in tmap would influence the final graph. We will demonstrate how these parameters change would affect the final graph and how to choose them.",
            "title": "How to choose parameters in tmap"
        },
        {
            "location": "/param/#selecting-components-in-filter",
            "text": "The primary parameter to effect the resulting is the  components  in  filter . If we explore the algorithms of  tmap  in  how tmap work , we could know that increasing the number components of filter will exponential increase the time of calculation and also the number of edges/nodes. Besides, changing the way of projection or chosen components is altering the angle of view to data points clouds and it will also impact the final graph we extracted from original data.  from sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn import datasets\nfrom sklearn.cluster import DBSCAN\nfrom tmap.tda import mapper, filter\nfrom tmap.tda.cover import Cover\nfrom tmap.tda.plot import show, Color\n\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Step1. initiate a Mapper\ntm = mapper.Mapper(verbose=1)\n\n# Step2. Projection\nlens = [filter.MDS(components=[0, 1],random_state=100)]\nprojected_X = tm.filter(X, lens=lens)\n\n# Step3. Covering, clustering & mapping\nclusterer = DBSCAN(eps=0.75, min_samples=1)\ncover = Cover(projected_data=MinMaxScaler().fit_transform(projected_X), resolution=20, overlap=0.75)\ngraph = tm.map(data=StandardScaler().fit_transform(X), cover=cover, clusterer=clusterer)\n\nshow(data=X,graph=graph, color='b', fig_size=(10, 10), node_size=15, mode='spring', strength=0.03)  Filtering by MDS.\n...calculate distance matrix using the euclidean metric.\nFinish filtering of points cloud data.\nMapping on data (150, 4) using lens (150, 2)\n...minimal number of points in hypercube to do clustering: 1\n...create 219 nodes.\n...calculate projection coordinates of nodes.\n...construct a TDA graph.\n...create 1133 edges.\nFinish TDA mapping  \nUsing first two components of MDS as  lens , graph with 219 nodes ans 1133 edges will be generated.  What if we choose first three components of MDS?  from sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn import datasets\nfrom sklearn.cluster import DBSCAN\nfrom tmap.tda import mapper, filter\nfrom tmap.tda.cover import Cover\nfrom tmap.tda.plot import show\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Step1. initiate a Mapper\ntm = mapper.Mapper(verbose=1)\n\n# Step2. Projection\nlens = [filter.MDS(components=[0, 1,2],random_state=100)]\nprojected_X = tm.filter(X, lens=lens)\n\n# Step3. Covering, clustering & mapping\nclusterer = DBSCAN(eps=0.75, min_samples=1)\ncover = Cover(projected_data=MinMaxScaler().fit_transform(projected_X), resolution=20, overlap=0.75)\ngraph = tm.map(data=StandardScaler().fit_transform(X), cover=cover, clusterer=clusterer)\n\nshow(data=X,graph=graph, color='b', fig_size=(10, 10), node_size=15, mode='spring', strength=0.17)  Filtering by MDS.\n...calculate distance matrix using the euclidean metric.\nFinish filtering of points cloud data.\nMapping on data (150, 4) using lens (150, 3)\n...minimal number of points in hypercube to do clustering: 1\n...create 921 nodes.\n...calculate projection coordinates of nodes.\n...construct a TDA graph.\n...create 13279 edges.\nFinish TDA mapping   We could get a graph with 921 nodes and 13279 edges. Increasing components would make the graph become bigger and redundant. If we visualize these two graphs, we could see that the general topological structure is similar. The more the number of  components  you provide, the more complicated graph will be generated because of the dimensional problems. We recommend the default parameter  components=[0,1]  in most situations.",
            "title": "Selecting components in filter"
        },
        {
            "location": "/param/#selecting-resolution-and-overlap-in-cover",
            "text": "A further parameter that affects the resulting is about the  Cover  which is a sifter for any given filter.  resolution  decides how many intervals you need to extract from the original point cloud based on the  filter  in each axis.  overlap  influences how much space between each interval need to be overlapped.  For  resolution , the number of hypercube equals to  resolution ** n_components . Enlarging the parameter will highly increase the time of calculation but improve the precision of the graph. Theoretically, higher precision is better for retrieving more detail in the graph. But blindly enlarging  resolution  could get a catastrophe that the number of nodes is much more than the number of samples and no connections will emerge between any of nodes.   If we enlarge 10 times of the  resolution   and leave the  overlap  unchanged will cause the situation above. High  resolution  for a small data set will cause each bin/hypercube has too few samples before clustering and no samples are located inside the overlap region. The resulting graph will become a network of discrete islands.  The smaller the  resolution  of cover, the less the graph could describe original points cloud. A suitable  resolution  could highly reduce the time and effort.  For  overlap , it majority influences the edges between each node. The larger the  overlap  you provide, the more edges will emerge until reaching limitation. But too low  overlap  will lose some detail of origin data.   As we see above,  resolution  is an empirical param which is depended on the size of your data set. It should be carefully adjusted to depict the data set properly. The larger the  overlap  you provide, the more structure it will capture. But for visualization and calculation, we should choose a suitable value.  Apart from the general structure of the graph, these parameters also influence most  cover ratio  which is discussed below.",
            "title": "Selecting resolution and overlap in Cover"
        },
        {
            "location": "/param/#selecting-eps-and-min_samples-in-dbscan",
            "text": "In tmap, default cluster called DBSCAN which requires 2 parameters -  eps , which restrict maximum distance between two samples for them to be considered as in the same cluster; and  min_samples , which specifies how many neighbors a point should have to be included into a cluster. However, you may not know these values in advance.  Here we implemented a function called  utils.optimize_dbscan_eps  to easily adjust  eps .  For better performances, we should enlarge the  eps  to cover most of the pairs. To achieve this, the shortest distance from each point to its nearest neighbor will be considered together. Collecting all distance could build a distribution and using a  threshold  as a  percentile  to cut how many pairwise distance you want to cover. After passing a threshold to  optimize_dbscan_eps , you could get a suitable  eps  for DBSCAN algorithm.  After we have chosen the value for  eps  , we may wonder how many points lie within each point's epsilon-neighborhood.  For extracting the topological properties as much as possible, we should still consider the distribution construct by the number of neighbor of each point. But not like the  eps  optimization function, we have not implemented this function yet. It may be implemented in the future.",
            "title": "Selecting eps and min_samples in DBSCAN"
        },
        {
            "location": "/param/#optimization-of-cover-ratio",
            "text": "cover ratio  is a variant for measuring how many samples are retaining or missing during the transform process from data to the graph.  During the process, samples placed in a hyper cube without being clustered into any nodes will be discarded. Usually, it is taking as outlier or noise found in data.  If we don't consider  cover ratio  in tmap result, it could get a perfect result like overfitting when we drop too much samples. So  cover ratio  need to be considered",
            "title": "Optimization of cover ratio"
        },
        {
            "location": "/param/#relationship-between-cover-ratio-with-other-parameters",
            "text": "Given different parameters to FGFP data in example would generate under figure.  Apparently, different parameters own different relationship with  cover ratio  and it is monotonous in given range.     paramerters  relationship      eps threshold  monotonous increasing    overlap  monotonous increasing    resolution  monotonous decreasing    min samples  monotonous decreasing",
            "title": "relationship between cover ratio with other parameters"
        },
        {
            "location": "/param/#summarize",
            "text": "cover ratio  is a variant which will be impacted by every parameters and so it is hard to tell what influence it will make instead of other parameters. In general, it may cover at least 80% samples like the explained proportion of PCA.  In the case of  cover ratio  of 80%, resolution may reach  sqrt(n_samples*2)  as far as possible. Other parameters may be chosen depending on different circumstances. Note that there are not best parameters in tmap because different features may present different topological shape in high dimensional space. Using different angle of view and resulting a weird graph doesn't mean that it is a wrong graph. How to benchmark the graph properly also is open to exploration and implement in the future.",
            "title": "Summarize"
        },
        {
            "location": "/vis/",
            "text": "Basic Usage and how legend work\n\n\nAfter resulting in the graph from the mapper, visualization is the main task we should achieve. Mapper of tda is an algorithm for clustering and feature selection. And visualization is the best way to showing the clustering result and revealing the potential topological structure from a graph.\n\n\nFor now, all visualization in tmap are based on the \nmatplotlib\n and \nnetworkx\n. Interactive web-based visualization would come out as soon as possible.\n\n\nBack to the basic usage of the visualization, if we got a graph from below codes.\n\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn import datasets\nfrom sklearn.cluster import DBSCAN\nfrom tda import mapper, filter\nfrom tda.cover import Cover\nfrom tda.plot import show, Color\n\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Step1. initiate a Mapper\ntm = mapper.Mapper(verbose=1)\n\n# Step2. Projection\nlens = [filter.MDS(components=[0, 1],random_state=100)]\nprojected_X = tm.filter(X, lens=lens)\n\n# Step3. Covering, clustering & mapping\nclusterer = DBSCAN(eps=0.75, min_samples=1)\ncover = Cover(projected_data=MinMaxScaler().fit_transform(projected_X), resolution=20, overlap=0.75)\ngraph = tm.map(data=StandardScaler().fit_transform(X), cover=cover, clusterer=clusterer)\n\n\n\n\nAfter generating the graph, we could simply use \nshow\n to visualize it without any grouping information.\n\n\nshow(data=X, graph=graph, fig_size=(10, 10), node_size=15, mode='spring', strength=0.04)\n\n\n\n\n\n\nVisualization without grouping information is hard to representing any thing from result. If we want to add any grouping information for this graph, we just need to pass a array/list to \ncolor\n and create a object as the param of \nshow\n.\n\n\ncolor = Color(target=y, dtype=\"categorical\")\nshow(data=X, graph=graph, color=color, fig_size=(10, 10), node_size=15, mode='spring', strength=0.04)\n\n\n\n\n\n\ncolor = Color(target=y, dtype=\"numerical\")\nshow(data=X, graph=graph, color=color, fig_size=(10, 10), node_size=15, mode='spring', strength=0.04)\n\n\n\n\n\n\nIn \ncolor\n, we should choose which kind of data you want to display. If you provide a categorical data, you could use \ndtype='categorical'\n to display three colors for clustering or you could use \ndtype=\"numerical\"\n to visualize the transition of each categorical.\n\n\nThe \ntarget\n of \ncolor\n doesn't constrained of the source but restrict by the shape. The array passing to \ntarget\n should have length equal number of samples or number of nodes.\n\n\nVisualization with importance of features.\n\n\nAfter basic usage of visualization, we need to go further to explore what can we do at microbiome with this tool. Here is the example with 16s data from European.\nRef 4\n. It also is the test set of tmap.",
            "title": "Visulization"
        },
        {
            "location": "/vis/#basic-usage-and-how-legend-work",
            "text": "After resulting in the graph from the mapper, visualization is the main task we should achieve. Mapper of tda is an algorithm for clustering and feature selection. And visualization is the best way to showing the clustering result and revealing the potential topological structure from a graph.  For now, all visualization in tmap are based on the  matplotlib  and  networkx . Interactive web-based visualization would come out as soon as possible.  Back to the basic usage of the visualization, if we got a graph from below codes.  from sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn import datasets\nfrom sklearn.cluster import DBSCAN\nfrom tda import mapper, filter\nfrom tda.cover import Cover\nfrom tda.plot import show, Color\n\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Step1. initiate a Mapper\ntm = mapper.Mapper(verbose=1)\n\n# Step2. Projection\nlens = [filter.MDS(components=[0, 1],random_state=100)]\nprojected_X = tm.filter(X, lens=lens)\n\n# Step3. Covering, clustering & mapping\nclusterer = DBSCAN(eps=0.75, min_samples=1)\ncover = Cover(projected_data=MinMaxScaler().fit_transform(projected_X), resolution=20, overlap=0.75)\ngraph = tm.map(data=StandardScaler().fit_transform(X), cover=cover, clusterer=clusterer)  After generating the graph, we could simply use  show  to visualize it without any grouping information.  show(data=X, graph=graph, fig_size=(10, 10), node_size=15, mode='spring', strength=0.04)   Visualization without grouping information is hard to representing any thing from result. If we want to add any grouping information for this graph, we just need to pass a array/list to  color  and create a object as the param of  show .  color = Color(target=y, dtype=\"categorical\")\nshow(data=X, graph=graph, color=color, fig_size=(10, 10), node_size=15, mode='spring', strength=0.04)   color = Color(target=y, dtype=\"numerical\")\nshow(data=X, graph=graph, color=color, fig_size=(10, 10), node_size=15, mode='spring', strength=0.04)   In  color , we should choose which kind of data you want to display. If you provide a categorical data, you could use  dtype='categorical'  to display three colors for clustering or you could use  dtype=\"numerical\"  to visualize the transition of each categorical.  The  target  of  color  doesn't constrained of the source but restrict by the shape. The array passing to  target  should have length equal number of samples or number of nodes.",
            "title": "Basic Usage and how legend work"
        },
        {
            "location": "/vis/#visualization-with-importance-of-features",
            "text": "After basic usage of visualization, we need to go further to explore what can we do at microbiome with this tool. Here is the example with 16s data from European. Ref 4 . It also is the test set of tmap.",
            "title": "Visualization with importance of features."
        },
        {
            "location": "/association/",
            "text": "Assoication in tmap\n\n\nAfter visualizing SAFE score of feature, we could observe some share and similar distribution between some features. From the distribution, we could see the contribution of a feature to whole network constructed by complete data.\n\n\nBase on the network and SAFE score, we could calculate the association between features and extract the above relationship. \nNote that, it isn't a traditional association because it is calculated by comparing their SAFE distribution in network.\n\n\nHighly correlated features\n\n\nSAFE scores distribution between \nunclassified_Clostridiaceae\n and \nMethanobrevibacter\n in \ntest/test_FGFP.py\n.\n\n\n\nuncorrelated features\n\n\nSAFE scores distribution between \nPrevotella\n and \nBacteroides\n in \ntest/test_FGFP.py\n.\n\n\n\nBasic usage",
            "title": "Association"
        },
        {
            "location": "/association/#assoication-in-tmap",
            "text": "After visualizing SAFE score of feature, we could observe some share and similar distribution between some features. From the distribution, we could see the contribution of a feature to whole network constructed by complete data.  Base on the network and SAFE score, we could calculate the association between features and extract the above relationship.  Note that, it isn't a traditional association because it is calculated by comparing their SAFE distribution in network.",
            "title": "Assoication in tmap"
        },
        {
            "location": "/association/#highly-correlated-features",
            "text": "SAFE scores distribution between  unclassified_Clostridiaceae  and  Methanobrevibacter  in  test/test_FGFP.py .",
            "title": "Highly correlated features"
        },
        {
            "location": "/association/#uncorrelated-features",
            "text": "SAFE scores distribution between  Prevotella  and  Bacteroides  in  test/test_FGFP.py .",
            "title": "uncorrelated features"
        },
        {
            "location": "/association/#basic-usage",
            "text": "",
            "title": "Basic usage"
        },
        {
            "location": "/how2work/",
            "text": "How tmap work\n\n\nTopological Data Analysis, or TDA, is the currently popular nomenclature for\nthe set of techniques surrounding persistence, persistent homology, and the extraction\nof significant topological features from data. TDA is used to study complex high dimensional data sets by extracting shapes (patterns) and obtaining insights about them.\n\n\nAn especially underused TDA technique is \u201cmapper\u201d. I found this particularly useful for visualization, and I wonder why it isn\u2019t as widely applied as, say, t-SNE. Mapper fits a simplicial complex to data (often, just a graph), but in a very flexible way. The goal of mapper is either data visualization or clustering.\n\n\n\n\nSuppose that we start with a set of points. For a given \nr > 0\n, we construct the balls of radius \nr\n around each point of this set. We then consider an abstract collection of vertices corresponding to the given points. We \nconnect the vertices\n pairwise by an edge if the balls intersect each other.\n\n\nThe nerve theorem states that, under certain conditions, the topological invariants of the union of balls of a cover coincide with those of the \u010cech complex. This complex is thus an enlargement of the point cloud and can be used to glean its topological properties.\nRef 1\n\n\nPipelines of mapper\n\n\nLet we introduce basic pipelines in our tools.\n\n\n\nWhen we get a real world data set, first we need to choose a distance metric to describe the distance between each point. In above graph, The metric is three-dimensional Euclidean distance, and the \nfilter\n function is the x-coordinate which is already colored by the filter function values.\n\n\nBy using the collection of intervals with two resolution parameters (\nresolution\n: a number N of intervals and \noverlap\n:, a percent overlap), the data set is binned into groups, whose filter values lie within a single interval, giving rise to a collection of overlapping bins called hypercube.\n\n\nBecause we chose the intervals to be overlapping with its neighbor, the binned data represents a systematic oversampling of the original data.\n\n\nThe final step of this pipeline is clustering. In each cube or overlapping bins, we could cluster all points with its \norigin distance\n with specific clustering function. After clustering, each cluster becomes a node or a vertex in the graph. We connect two clusters with an edge if they have one or more samples in common.\nRef 2\n\n\nHow SAFE work\n\n\nOriginally, spatial analysis of functional enrichment (SAFE) is a systematic method for annotating biological networks and examining their functional organization. \nRef 3\n\n\n\n\nGiven a graph and its adjacency matrix(Of course we could easily get this matrix).\n\n\nFor each node in the graph, SAFE defines the local neighborhood of X by identifying all other nodes located closer than a maximum distance threshold. By default, node distance is measured using the \nweighted shortest path length\n or using the \nunweighted shortest path length\n; However, other distance measures are also available\n\n\nBy default, the maximum distance threshold d equals to the 0.5th-percentile of all pair-wise node\ndistances in the network.\n\n\nFor each neighborhood, SAFE sums the product of neighbors values for a functional attribute of interest and the degree of the neighbors as a neighborhood score \nP\n. The score is then compared to the distribution of \nI\n random neighborhood scores obtained by reshuffling the samples in the network and, consequently, each of the functional attribute value \nindependently\n. The significance of the enrichment is determined as the probability that a single observation from random distribution will fall into the interval between origin neighborhood score and infinite.\n\n\nConvert neighborhood significance p-values into neighborhood enrichment scores \nO\n, normalized to a range from 0 to 1, by computing:\n\n\nwhere \nI\n is the iterations times, \nP\n is the neighborhood score between vertex i and vertex j, \nO\n is the neighborhood enrichment score between vertex i and vertex j. This equation focuses on one specific feature.\n\n\n\n\nA node is considered significantly enriched for the features if:\n\n\n\n\n\n\n\nUsing \nnumber of significant nodes\n or \nsum of SAFE score of significant nodes\n to ranking features.\n\n\n\n\n\n\nIn \nnetx.SAFE\n, we implement two major functions \nSAFE_by_nodes\n and \nSAFE_by_samples\n. The major difference between them is the object of permutation. Former is shuffling by each node which mean all samples in one nodes will shuffle together. Latter is shuffling each sample which mean all samples will randomly located at any nodes.\n\n\nThe result of these two function has little bit difference. Which function should be chosen is dependent on what you need.",
            "title": "How TMAP work"
        },
        {
            "location": "/how2work/#how-tmap-work",
            "text": "Topological Data Analysis, or TDA, is the currently popular nomenclature for\nthe set of techniques surrounding persistence, persistent homology, and the extraction\nof significant topological features from data. TDA is used to study complex high dimensional data sets by extracting shapes (patterns) and obtaining insights about them.  An especially underused TDA technique is \u201cmapper\u201d. I found this particularly useful for visualization, and I wonder why it isn\u2019t as widely applied as, say, t-SNE. Mapper fits a simplicial complex to data (often, just a graph), but in a very flexible way. The goal of mapper is either data visualization or clustering.   Suppose that we start with a set of points. For a given  r > 0 , we construct the balls of radius  r  around each point of this set. We then consider an abstract collection of vertices corresponding to the given points. We  connect the vertices  pairwise by an edge if the balls intersect each other.  The nerve theorem states that, under certain conditions, the topological invariants of the union of balls of a cover coincide with those of the \u010cech complex. This complex is thus an enlargement of the point cloud and can be used to glean its topological properties. Ref 1",
            "title": "How tmap work"
        },
        {
            "location": "/how2work/#pipelines-of-mapper",
            "text": "Let we introduce basic pipelines in our tools.  When we get a real world data set, first we need to choose a distance metric to describe the distance between each point. In above graph, The metric is three-dimensional Euclidean distance, and the  filter  function is the x-coordinate which is already colored by the filter function values.  By using the collection of intervals with two resolution parameters ( resolution : a number N of intervals and  overlap :, a percent overlap), the data set is binned into groups, whose filter values lie within a single interval, giving rise to a collection of overlapping bins called hypercube.  Because we chose the intervals to be overlapping with its neighbor, the binned data represents a systematic oversampling of the original data.  The final step of this pipeline is clustering. In each cube or overlapping bins, we could cluster all points with its  origin distance  with specific clustering function. After clustering, each cluster becomes a node or a vertex in the graph. We connect two clusters with an edge if they have one or more samples in common. Ref 2",
            "title": "Pipelines of mapper"
        },
        {
            "location": "/how2work/#how-safe-work",
            "text": "Originally, spatial analysis of functional enrichment (SAFE) is a systematic method for annotating biological networks and examining their functional organization.  Ref 3   Given a graph and its adjacency matrix(Of course we could easily get this matrix).  For each node in the graph, SAFE defines the local neighborhood of X by identifying all other nodes located closer than a maximum distance threshold. By default, node distance is measured using the  weighted shortest path length  or using the  unweighted shortest path length ; However, other distance measures are also available  By default, the maximum distance threshold d equals to the 0.5th-percentile of all pair-wise node\ndistances in the network.  For each neighborhood, SAFE sums the product of neighbors values for a functional attribute of interest and the degree of the neighbors as a neighborhood score  P . The score is then compared to the distribution of  I  random neighborhood scores obtained by reshuffling the samples in the network and, consequently, each of the functional attribute value  independently . The significance of the enrichment is determined as the probability that a single observation from random distribution will fall into the interval between origin neighborhood score and infinite.  Convert neighborhood significance p-values into neighborhood enrichment scores  O , normalized to a range from 0 to 1, by computing: \nwhere  I  is the iterations times,  P  is the neighborhood score between vertex i and vertex j,  O  is the neighborhood enrichment score between vertex i and vertex j. This equation focuses on one specific feature.   A node is considered significantly enriched for the features if:    Using  number of significant nodes  or  sum of SAFE score of significant nodes  to ranking features.    In  netx.SAFE , we implement two major functions  SAFE_by_nodes  and  SAFE_by_samples . The major difference between them is the object of permutation. Former is shuffling by each node which mean all samples in one nodes will shuffle together. Latter is shuffling each sample which mean all samples will randomly located at any nodes.  The result of these two function has little bit difference. Which function should be chosen is dependent on what you need.",
            "title": "How SAFE work"
        },
        {
            "location": "/example/",
            "text": "Basic examples\n\n\nMNIST datasets\n\n\nMicrobial 16s data",
            "title": "Demonstrate a few examples"
        },
        {
            "location": "/example/#basic-examples",
            "text": "",
            "title": "Basic examples"
        },
        {
            "location": "/example/#mnist-datasets",
            "text": "",
            "title": "MNIST datasets"
        },
        {
            "location": "/example/#microbial-16s-data",
            "text": "",
            "title": "Microbial 16s data"
        },
        {
            "location": "/api/",
            "text": "Mapper\n\n\nCover\n\n\nFilters",
            "title": "API Reference"
        },
        {
            "location": "/api/#mapper",
            "text": "",
            "title": "Mapper"
        },
        {
            "location": "/api/#cover",
            "text": "",
            "title": "Cover"
        },
        {
            "location": "/api/#filters",
            "text": "",
            "title": "Filters"
        },
        {
            "location": "/reference/",
            "text": "Reference\n\n\n\n\nEdelsbrunner H, Harer J. Computational topology: an introduction[M]. American Mathematical Soc., 2010.\n\n\nLum P Y, Singh G, Lehman A, et al. Extracting insights from the shape of complex data using topology[J]. Scientific reports, 2013, 3: srep01236.\n\n\nBaryshnikova A. Systematic functional annotation and visualization of biological networks[J]. Cell systems, 2016, 2(6): 412-421.\n\n\nFalony, Gwen, et al. \"Population-level analysis of gut microbiome variation.\" Science 352.6285 (2016): 560-564.",
            "title": "Reference"
        },
        {
            "location": "/reference/#reference",
            "text": "Edelsbrunner H, Harer J. Computational topology: an introduction[M]. American Mathematical Soc., 2010.  Lum P Y, Singh G, Lehman A, et al. Extracting insights from the shape of complex data using topology[J]. Scientific reports, 2013, 3: srep01236.  Baryshnikova A. Systematic functional annotation and visualization of biological networks[J]. Cell systems, 2016, 2(6): 412-421.  Falony, Gwen, et al. \"Population-level analysis of gut microbiome variation.\" Science 352.6285 (2016): 560-564.",
            "title": "Reference"
        },
        {
            "location": "/1/",
            "text": "",
            "title": "License"
        },
        {
            "location": "/2/",
            "text": "",
            "title": "Release Notes"
        },
        {
            "location": "/FAQ/",
            "text": "FAQ\n\n\n\n\n\n\nWhy can't I find some samples in the resulting graph?\n\n\nDuring the process of tmap, it will drop some samples which is lack of required number of neighbors. Usually, these samples is taken as outlier or noise which is need to be obsoleted. If you are worried about the number of samples retaining in the graph, you could use the function called \ntmap.tda.utils.cover_ratio\n to calculate the cover ratio. Usually, cover ratio higher than 80% is basic need for tmap result.\n\n\n\n\n\n\nThere are too much pairs which is significant co-enriched found in the result of coenrich. How could I choose from them?\n\n\nIn the FGFP example, it could see that it do have too much pairs found in the result of coenrich. The scores or p-value of co-enrich pairs are calculated base on the graph. Significant coenrichment is defined as pairs of features that share similar distribution along with the graph by SAFE score.\n\n\nCurrently, we don't have better way to filter it again and extract more significant pairs. We could use several graphs with different parameters to find significant pairs under individual situations. We could also use p-value of coenrich to rank all pairs and extract the top.\n\n\n\n\n\n\nWhy would I choose tmap instead of other traditional ordination analysis?\n\n\nblabla....\n\n\n\n\n\n\nWhat does the meaning of SAFE score?\n\n\nblabla....\n1. \nblabla...\n\n\nblabla....",
            "title": "FAQ"
        },
        {
            "location": "/FAQ/#faq",
            "text": "Why can't I find some samples in the resulting graph?  During the process of tmap, it will drop some samples which is lack of required number of neighbors. Usually, these samples is taken as outlier or noise which is need to be obsoleted. If you are worried about the number of samples retaining in the graph, you could use the function called  tmap.tda.utils.cover_ratio  to calculate the cover ratio. Usually, cover ratio higher than 80% is basic need for tmap result.    There are too much pairs which is significant co-enriched found in the result of coenrich. How could I choose from them?  In the FGFP example, it could see that it do have too much pairs found in the result of coenrich. The scores or p-value of co-enrich pairs are calculated base on the graph. Significant coenrichment is defined as pairs of features that share similar distribution along with the graph by SAFE score.  Currently, we don't have better way to filter it again and extract more significant pairs. We could use several graphs with different parameters to find significant pairs under individual situations. We could also use p-value of coenrich to rank all pairs and extract the top.    Why would I choose tmap instead of other traditional ordination analysis?  blabla....    What does the meaning of SAFE score?  blabla....\n1.  blabla...  blabla....",
            "title": "FAQ"
        }
    ]
}